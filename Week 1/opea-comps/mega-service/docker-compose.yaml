services:
  chatmegaservice:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chatmegaservice
    ports:
      - "8888:8888"
    environment:
      - LLM_SERVER_HOST_IP=ollama-server
      - LLM_SERVER_PORT=11434
      - OTEL_SDK_DISABLED=true
      - MEGA_SERVICE_PORT=8888
      - LLM_MODEL=llama3.2:3b
      - OTEL_SDK_DISABLED=true
      - OTEL_TRACES_EXPORTER=none
    depends_on:
      - ollama-server
    networks:
      - opea-network
    ipc: host
    restart: always

  ollama-server:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:11434
    environment:
      - no_proxy=${no_proxy}
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
      - LLM_MODEL_ID=${LLM_MODEL_ID}
      - host_ip=${host_ip}
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - opea-network
    command: serve

volumes:
  ollama-data:
    driver: local

networks:
  opea-network:
    driver: bridge